"""
Stage 6: Executive Polish
Model: Claude Haiku (cheap but good writing)
Cost: ~$0.015 per call
"""

import json
import logging
from typing import Dict, Any

from app.services.analysis.llm_client import call_llm_with_retry
from app.core.model_config import get_model_for_stage, get_stage_config

logger = logging.getLogger(__name__)

MODEL_POLISH = get_model_for_stage("polish")


async def stage6_executive_polish(
    company: str,
    industry: str,
    strategic_analysis: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Stage 6: Polish report for executive readability
    Model: Claude Haiku (cheap but good writing)
    Cost: ~$0.015 per call

    Args:
        company: Company name
        industry: Industry sector (for context, though not heavily used in this stage)
        strategic_analysis: Output from stage 3
    """

    logger.info("[STAGE 6] Polishing report for executive readability...")

    prompt = f"""# EXECUTIVE POLISH TASK

You are an executive communications specialist. Polish this strategic analysis for C-level readability.

## Current Analysis (Generated by Strategy Team)
{json.dumps(strategic_analysis, indent=2, ensure_ascii=False)}

---

# YOUR TASKS

## 1. Enhance Executive Summary
Make it compelling, concise, impactful:
- Start with a powerful opening (current situation in 1 sentence)
- Highlight TOP 3 most important insights
- State TOP 3 priority actions with expected impact
- Keep to 3-4 paragraphs max

## 2. Improve Readability
- Fix any awkward phrasing
- Ensure consistent tone (executive, direct, confident)
- Remove jargon unless industry-standard
- Add clarity where ambiguous

## 3. Enhance Recommendations
For each recommendation:
- Make title punchy and clear
- Ensure "como_implementar" steps are specific and actionable
- Verify investment and ROI numbers are realistic
- Add urgency indicators (quick win vs long-term)

## 4. Improve Scenario Planning
Make scenarios vivid and memorable:
- Add specific trigger dates/events
- Make actions concrete
- Quantify impacts clearly

## 5. Quality Check
- Remove any placeholder text
- Ensure all numbers are realistic (not "R$ 0" or "100%")
- Check that OKRs are truly measurable
- Verify Brazilian Portuguese is natural and professional

## 6. **CRITICAL - Preserve Source Attribution**
- **NEVER remove source markers** from quantitative claims
- Keep all "(fonte: X)", "(baseado em Y)", "(estimativa Z)" markers INTACT
- If text says "R$ 100 milhões (fonte: Website)", DO NOT change it to "R$ 100 milhões"
- Source attribution is MANDATORY for credibility - preserve it at all costs

---

# OUTPUT FORMAT (JSON ONLY)

Return the SAME JSON structure as input, but with polished text.

**DO NOT:**
- Change the structure or remove keys
- Add new sections
- Alter numbers/data significantly
- Make it longer (aim for clarity, not verbosity)
- **REMOVE SOURCE ATTRIBUTION** - Keep all "(fonte: X)" markers

**DO:**
- Improve clarity and flow
- Make language more executive-friendly
- Ensure actionability
- Fix any errors or awkwardness
- **Preserve all source markers and attributions**

Return JSON only. No markdown, no explanations.
"""

    system_prompt = "You are an expert at executive communications. Polish for clarity and impact. Preserve structure and data."

    usage_stats = {}
    try:
        response, usage_stats = await call_llm_with_retry(
            stage_name="STAGE 6",
            model=MODEL_POLISH,
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=0.5,  # Moderate creativity
            max_tokens=10000
        )
        polished_analysis = json.loads(response)

    except Exception as e:
        logger.warning(f"[STAGE 6] Primary model failed, trying FREE fallback model...")
        logger.warning(f"[STAGE 6] Error: {str(e)}")

        # Fallback: Use free model (Gemini Flash Free)
        stage_config = get_stage_config("polish")
        free_fallback = stage_config.get("free_fallback_model", "google/gemini-2.0-flash-exp:free")

        response, usage_stats = await call_llm_with_retry(
            stage_name="STAGE 6 (FREE FALLBACK)",
            model=free_fallback,
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=0.5,
            max_tokens=10000
        )
        polished_analysis = json.loads(response)

    logger.info(f"[STAGE 6] ✅ Report polished for executive readability")

    # Add usage stats to result
    polished_analysis["_usage_stats"] = usage_stats
    return polished_analysis
