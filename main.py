"""
FastAPI Backend for Strategy AI Lead Generator
With Supabase, Authentication, Apify Integration, and Upstash Redis
"""
from fastapi import FastAPI, BackgroundTasks, HTTPException, Request, Depends, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from contextlib import asynccontextmanager
from typing import Dict, List, AsyncIterator
from datetime import datetime, timedelta, timezone
import json
import os
import asyncio
from dotenv import load_dotenv

# Import local modules
from models import (
    SubmissionCreate,
    SubmissionResponse,
    Submission,
    SubmissionsListResponse,
    ReprocessResponse,
    ErrorResponse,
    LoginRequest,
    LoginResponse,
    TokenResponse,
    UserResponse,
    SignupRequest,
    SignupResponse,
    EditRequest,
    EditResponse,
    ApplyEditRequest,
    ApplyEditResponse,
    RegeneratePDFResponse,
    UpdateStatusRequest,
    UpdateStatusResponse,
)
from database import (
    init_db,
    create_submission,
    get_submission,
    get_all_submissions,
    update_submission_status,
)
from analysis_enhanced import generate_enhanced_analysis, validate_enhanced_analysis
from analysis_multistage import generate_multistage_analysis  # NEW: Multi-stage pipeline
from auth import authenticate_user, get_current_user, RequireAuth
from rate_limiter import check_rate_limit
from apify_service import gather_all_apify_data
from perplexity_service import comprehensive_market_research
import perplexity_service  # Need module for follow-up research
from dashboard_intelligence import generate_dashboard_intelligence  # NEW: FREE dashboard AI
from ai_editor import (  # NEW: AI-powered report editor
    generate_edit_suggestion,
    generate_edit_suggestion_streaming,
    apply_edit_to_json_path,
    extract_section_context,
)
from pdf_generator import generate_pdf_from_report  # PDF generation
from confidence_scorer import calculate_confidence_score  # AI confidence scoring
from ai_chat import send_chat_message, get_quick_prompts  # AI chat system

load_dotenv()

# Configuration
ALLOWED_ORIGINS = os.getenv(
    "ALLOWED_ORIGINS",
    "http://localhost:3000,http://localhost:3001"
).split(",")


# Lifespan context manager for startup/shutdown
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    print("[STARTUP] Starting Strategy AI Backend with Supabase + Apify + Auth...")
    await init_db()
    print("[OK] Database connection established")
    yield
    # Shutdown
    print("[SHUTDOWN] Shutting down...")


# Initialize FastAPI app
app = FastAPI(
    title="Strategy AI Lead Generator API",
    description="AI-powered business analysis lead generation system with Supabase, Auth, and Apify",
    version="2.0.0",
    lifespan=lifespan,
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# GZip Compression Middleware (compress responses > 1KB)
app.add_middleware(GZipMiddleware, minimum_size=1000)


# ============================================================================
# PROGRESS TRACKING FOR SSE STREAMING
# ============================================================================

# In-memory progress tracker (maps submission_id -> progress updates)
_progress_tracker: Dict[int, List[Dict]] = {}

def emit_progress(submission_id: int, stage: str, message: str, progress: int):
    """
    Emit progress update for SSE streaming

    Args:
        submission_id: The submission being processed
        stage: Current stage (e.g., "data_gathering", "ai_analysis", "completed")
        message: Human-readable progress message
        progress: Progress percentage (0-100)
    """
    if submission_id not in _progress_tracker:
        _progress_tracker[submission_id] = []

    update = {
        "stage": stage,
        "message": message,
        "progress": progress,
        "timestamp": datetime.utcnow().isoformat()
    }

    _progress_tracker[submission_id].append(update)
    print(f"[PROGRESS] Submission {submission_id}: {progress}% - {message}")


def get_progress_updates(submission_id: int) -> List[Dict]:
    """Get all progress updates for a submission"""
    return _progress_tracker.get(submission_id, [])


def clear_progress(submission_id: int):
    """Clear progress tracking for a submission (after completion)"""
    if submission_id in _progress_tracker:
        del _progress_tracker[submission_id]


# ============================================================================
# BACKGROUND TASK: Process AI Analysis with Progress Tracking
# ============================================================================

async def process_analysis_task(submission_id: int, force_regenerate: bool = False):
    """
    Background task to generate AI analysis with Apify enrichment

    Args:
        submission_id: ID of the submission to process
        force_regenerate: If True, bypass analysis cache and force fresh AI generation
                         (still uses institutional memory cache for external data)
    """
    try:
        # Get submission details
        submission = await get_submission(submission_id)
        if not submission:
            print(f"[ERROR] Submission {submission_id} not found")
            return

        print(f"[PROCESSING] Processing analysis for submission {submission_id}...")

        # Progress: Start
        emit_progress(submission_id, "initializing", f"Iniciando an√°lise para {submission['company']}", 0)

        # Step 1: Gather Apify data (web scraping, competitor research, trends, LinkedIn, news, social media)
        print(f"[APIFY] Gathering enrichment data for {submission['company']}...")
        emit_progress(submission_id, "data_gathering", "Coletando dados de 8 fontes (web, competidores, tend√™ncias, LinkedIn, not√≠cias, redes sociais)", 10)
        apify_data = None
        data_quality = {
            "website_scraped": False,
            "competitors_found": 0,
            "trends_researched": False,
            "company_enriched": False,
            "linkedin_company_found": False,
            "linkedin_founder_found": False,
            "news_found": False,
            "social_media_found": False,
            "sources_succeeded": 0,
            "sources_failed": 0,
            "failed_sources": [],
            "quality_tier": "minimal"
        }

        try:
            apify_data = await gather_all_apify_data(
                company=submission["company"],
                industry=submission["industry"],
                website=submission.get("website"),
                linkedin_company=submission.get("linkedin_company"),
                linkedin_founder=submission.get("linkedin_founder"),
                challenge=submission.get("challenge"),
            )
            print(f"[APIFY] Data gathering completed for submission {submission_id}")

            # LOG: Detailed breakdown of what each source returned
            print(f"[APIFY RESULTS] ===== DATA SOURCES BREAKDOWN =====")
            print(f"[APIFY RESULTS] Website Data: {'‚úÖ SUCCESS' if apify_data.get('website_data', {}).get('scraped_successfully') else '‚ùå FAILED'}")
            print(f"[APIFY RESULTS] Competitors: {apify_data.get('competitor_data', {}).get('competitors_found', 0)} found")
            print(f"[APIFY RESULTS] Industry Trends: {'‚úÖ SUCCESS' if apify_data.get('industry_trends', {}).get('researched_successfully') else '‚ùå FAILED'}")
            print(f"[APIFY RESULTS] Company Enrichment: {'‚úÖ SUCCESS' if apify_data.get('company_enrichment', {}).get('enriched_successfully') else '‚ùå FAILED'}")
            print(f"[APIFY RESULTS] LinkedIn Company: {'‚úÖ SUCCESS' if apify_data.get('linkedin_company_data', {}).get('scraped_successfully') else '‚ùå FAILED'}")
            print(f"[APIFY RESULTS] LinkedIn Founder: {'‚úÖ SUCCESS' if apify_data.get('linkedin_founder_data', {}).get('scraped_successfully') else '‚ùå FAILED'}")
            print(f"[APIFY RESULTS] News Articles: {'‚úÖ SUCCESS' if apify_data.get('news_data', {}).get('researched_successfully') else '‚ùå FAILED'}")
            print(f"[APIFY RESULTS] Social Media: {'‚úÖ SUCCESS' if apify_data.get('social_media_data', {}).get('researched_successfully') else '‚ùå FAILED'}")
            print(f"[APIFY RESULTS] =====================================")

            # Extract data quality metrics (now includes 8 data sources)
            if apify_data:
                # Core sources (original 4)
                data_quality["website_scraped"] = apify_data.get("website_data", {}).get("scraped_successfully", False)
                data_quality["competitors_found"] = apify_data.get("competitor_data", {}).get("competitors_found", 0)
                data_quality["trends_researched"] = apify_data.get("industry_trends", {}).get("researched_successfully", False)
                data_quality["company_enriched"] = apify_data.get("company_enrichment", {}).get("enriched_successfully", False)

                # NEW: LinkedIn sources
                data_quality["linkedin_company_found"] = apify_data.get("linkedin_company_data", {}).get("scraped_successfully", False)
                data_quality["linkedin_founder_found"] = apify_data.get("linkedin_founder_data", {}).get("scraped_successfully", False)

                # NEW: Public data sources
                data_quality["news_found"] = apify_data.get("news_data", {}).get("researched_successfully", False)
                data_quality["social_media_found"] = apify_data.get("social_media_data", {}).get("researched_successfully", False)

                # Count successes from all 8 sources
                successes = sum([
                    data_quality["website_scraped"],
                    data_quality["competitors_found"] > 0,
                    data_quality["trends_researched"],
                    data_quality["company_enriched"],
                    data_quality["linkedin_company_found"],
                    data_quality["linkedin_founder_found"],
                    data_quality["news_found"],
                    data_quality["social_media_found"]
                ])
                data_quality["sources_succeeded"] = successes
                data_quality["sources_failed"] = 8 - successes

                # Track failed sources
                if not data_quality["website_scraped"] and submission.get("website"):
                    data_quality["failed_sources"].append("website_scraping")
                if data_quality["competitors_found"] == 0:
                    data_quality["failed_sources"].append("competitor_research")
                if not data_quality["trends_researched"]:
                    data_quality["failed_sources"].append("industry_trends")
                if not data_quality["company_enriched"]:
                    data_quality["failed_sources"].append("company_enrichment")
                if not data_quality["linkedin_company_found"]:
                    data_quality["failed_sources"].append("linkedin_company")
                if not data_quality["linkedin_founder_found"] and submission.get("linkedin_founder"):
                    data_quality["failed_sources"].append("linkedin_founder")
                if not data_quality["news_found"]:
                    data_quality["failed_sources"].append("news_articles")
                if not data_quality["social_media_found"]:
                    data_quality["failed_sources"].append("social_media")

                # Calculate quality tier (8 sources total, more granular tiers)
                completion_rate = successes / 8
                if completion_rate >= 0.75:  # 6-8 sources = 75-100%
                    data_quality["quality_tier"] = "full"
                elif completion_rate >= 0.5:  # 4-5 sources = 50-74%
                    data_quality["quality_tier"] = "good"
                elif completion_rate >= 0.25:  # 2-3 sources = 25-49%
                    data_quality["quality_tier"] = "partial"
                else:  # 0-1 sources = 0-24%
                    data_quality["quality_tier"] = "minimal"

                data_quality["data_completeness"] = f"{int(completion_rate * 100)}%"

                # LOG: Final quality summary
                print(f"[DATA QUALITY] ===== FINAL SUMMARY =====")
                print(f"[DATA QUALITY] Sources Succeeded: {successes}/8")
                print(f"[DATA QUALITY] Quality Tier: {data_quality['quality_tier'].upper()}")
                print(f"[DATA QUALITY] Data Completeness: {data_quality['data_completeness']}")
                if data_quality["failed_sources"]:
                    print(f"[DATA QUALITY] Failed Sources: {', '.join(data_quality['failed_sources'])}")
                print(f"[DATA QUALITY] =====================================")

        except Exception as e:
            print(f"[WARNING] Apify enrichment failed: {str(e)}. Continuing with basic analysis...")
            apify_data = None
            data_quality["sources_failed"] = 8
            data_quality["failed_sources"] = ["all_sources_failed"]

        # Progress: Apify complete
        emit_progress(submission_id, "data_gathering", f"Dados coletados! {data_quality['sources_succeeded']}/8 fontes bem-sucedidas", 30)

        # Step 1.5: PERPLEXITY DEEP RESEARCH (LEGENDARY UPGRADE!)
        print(f"[PERPLEXITY] Starting comprehensive market research for {submission['company']}...")
        emit_progress(submission_id, "deep_research", "Iniciando pesquisa avan√ßada de mercado com IA (Perplexity)", 40)
        perplexity_data = None
        perplexity_success = False

        try:
            perplexity_data = await comprehensive_market_research(
                company=submission["company"],
                industry=submission["industry"],
                challenge=submission.get("challenge", "Expandir mercado e aumentar receita"),
                region="Brasil",
                specific_segment=None
            )

            perplexity_success = perplexity_data.get("research_completed", False)

            if perplexity_success:
                print(f"[PERPLEXITY] ‚úÖ Comprehensive research completed!")
                print(f"[PERPLEXITY] Success Rate: {perplexity_data.get('success_rate', 'N/A')}")

                # Add Perplexity as additional data sources to quality tracking
                perplexity_sources = perplexity_data.get('success_rate', '0/5').split('/')[0]
                data_quality["perplexity_sources"] = int(perplexity_sources)
                data_quality["sources_succeeded"] += int(perplexity_sources)

                # Update quality tier with Perplexity boost (now 8 Apify + 5 Perplexity = 13 total)
                total_sources = 13
                total_succeeded = data_quality["sources_succeeded"]
                completion_rate = total_succeeded / total_sources

                if completion_rate >= 0.75:  # 10-13 sources = 75-100%
                    data_quality["quality_tier"] = "legendary"  # NEW TIER!
                elif completion_rate >= 0.60:  # 8-9 sources = 60-74%
                    data_quality["quality_tier"] = "full"
                elif completion_rate >= 0.40:  # 5-7 sources = 40-59%
                    data_quality["quality_tier"] = "good"
                elif completion_rate >= 0.20:  # 3-4 sources = 20-39%
                    data_quality["quality_tier"] = "partial"
                else:  # 0-2 sources = 0-19%
                    data_quality["quality_tier"] = "minimal"

                data_quality["data_completeness"] = f"{int(completion_rate * 100)}%"

                print(f"[PERPLEXITY] üöÄ UPGRADED Quality Tier: {data_quality['quality_tier'].upper()}")
                print(f"[PERPLEXITY] Total Sources: {total_succeeded}/{total_sources} ({data_quality['data_completeness']})")
            else:
                print(f"[PERPLEXITY] ‚ö†Ô∏è Research partially completed or failed")

        except Exception as e:
            print(f"[WARNING] Perplexity research failed: {str(e)}. Continuing with Apify data only...")
            perplexity_data = None

        # Progress: Deep research complete
        emit_progress(submission_id, "deep_research", "Pesquisa de mercado conclu√≠da! Iniciando gera√ß√£o de an√°lise estrat√©gica", 50)

        # Step 2: Check cache FIRST before generating analysis (SAVES $15-25 PER HIT!)
        # Skip cache check if force_regenerate=True
        from enhanced_cache import get_cached_analysis, cache_analysis_result

        cached_analysis_result = None
        if not force_regenerate:
            print(f"[CACHE] üîç Checking analysis cache for {submission['company']}...")
            cached_analysis_result = await get_cached_analysis(
                company=submission["company"],
                industry=submission["industry"],
                challenge=submission.get("challenge"),
                website=submission.get("website")
            )
        else:
            print(f"[REGENERATE] üîÑ Force regenerate - bypassing analysis cache")

        if cached_analysis_result and cached_analysis_result.get("cache_hit") and not force_regenerate:
            # CACHE HIT! Return cached analysis instantly
            analysis = cached_analysis_result["analysis"]
            processing_time = 0.1  # Near-instant from cache
            cost_saved = cached_analysis_result.get("cost_saved", 20)
            cache_age_hours = cached_analysis_result.get("cache_age_hours", 0)

            print(f"[CACHE] üéØüí∞ CACHE HIT! Saved ${cost_saved:.2f} (age: {cache_age_hours:.1f}h)")
            emit_progress(submission_id, "ai_analysis", f"‚ú® An√°lise recuperada do cache (economia de ${cost_saved:.2f}!)", 60)

        else:
            # CACHE MISS or FORCE REGENERATE - Generate new analysis
            if force_regenerate:
                print(f"[REGENERATE] Generating fresh AI analysis (external data may be cached)")
            else:
                print(f"[CACHE] ‚ùå Cache miss - generating new analysis")
            print(f"[AI] üöÄ Generating LEGENDARY strategic analysis (6-stage pipeline) for submission {submission_id}...")
            emit_progress(submission_id, "ai_analysis", "Gerando an√°lise estrat√©gica com IA (pipeline de 6 etapas: SWOT, PESTEL, Competidores, Riscos, OKRs)", 60)

            import time
            start_time = time.time()

            # Use the NEW multi-stage pipeline with ALL advanced features
            analysis = await generate_multistage_analysis(
                company=submission["company"],
                industry=submission["industry"],
                website=submission.get("website"),
                challenge=submission.get("challenge"),
                apify_data=apify_data,
                perplexity_data=perplexity_data,  # Initial Perplexity research
                run_all_stages=True,  # Enable ALL 6 stages (gap analysis, competitive matrix, risk scoring)
                perplexity_service=perplexity_service  # For follow-up research queries
            )

            processing_time = time.time() - start_time

            # Cache the result for future use (CRITICAL - saves $15-25 next time!)
            estimated_cost = 20.0  # Approximate cost of full analysis
            await cache_analysis_result(
                company=submission["company"],
                industry=submission["industry"],
                challenge=submission.get("challenge"),
                website=submission.get("website"),
                analysis_result=analysis,
                cost=estimated_cost,
                processing_time=processing_time
            )
            print(f"[CACHE] ‚úÖ Analysis cached - will save ${estimated_cost:.2f} on next request")

        # Validate structure (the core structure is still the same)
        if not await validate_enhanced_analysis(analysis):
            print("[WARNING] Core validation failed, but multistage has additional sections - continuing...")
            # Multistage adds extra sections, so validation may fail on core-only check
            # This is expected and okay

        # CRITICAL FIX: Recalculate quality if Perplexity was removed during fallback
        used_perplexity = analysis.get("_metadata", {}).get("used_perplexity", True)
        if perplexity_data and not used_perplexity:
            # AI fallback happened - Perplexity data was NOT used in final report
            print(f"[DATA QUALITY] Recalculating quality tier (Perplexity removed in fallback)")

            # Remove Perplexity sources from count
            perplexity_sources = data_quality.get("perplexity_sources", 5)
            data_quality["sources_succeeded"] -= perplexity_sources
            data_quality["perplexity_sources"] = 0

            # Recalculate with only Apify (8 sources total)
            total_sources = 8
            total_succeeded = data_quality["sources_succeeded"]
            completion_rate = total_succeeded / total_sources

            if completion_rate >= 0.75:  # 6-8 sources = 75-100%
                data_quality["quality_tier"] = "full"
            elif completion_rate >= 0.5:  # 4-5 sources = 50-74%
                data_quality["quality_tier"] = "good"
            elif completion_rate >= 0.25:  # 2-3 sources = 25-49%
                data_quality["quality_tier"] = "partial"
            else:  # 0-1 sources = 0-24%
                data_quality["quality_tier"] = "minimal"

            data_quality["data_completeness"] = f"{int(completion_rate * 100)}%"

            print(f"[DATA QUALITY] ‚úÖ Recalculated: {total_succeeded}/{total_sources} sources = {data_quality['quality_tier'].upper()} ({data_quality['data_completeness']})")

        # Add data disclaimer to metadata if data is incomplete
        if data_quality["quality_tier"] in ["partial", "minimal"]:
            disclaimer = f"Este relat√≥rio foi gerado com {data_quality['data_completeness']} dos dados dispon√≠veis. "
            if data_quality["failed_sources"]:
                disclaimer += f"Fontes indispon√≠veis: {', '.join(data_quality['failed_sources'])}. "
            disclaimer += "A an√°lise foi feita com base nas informa√ß√µes p√∫blicas dispon√≠veis."
            analysis["_metadata"]["data_disclaimer"] = disclaimer

        # Extract processing metadata
        processing_meta = {
            "model": analysis.get("_metadata", {}).get("model_used", "gpt-4o"),
            "framework_version": analysis.get("_metadata", {}).get("framework_version", "10XMentorAI v2.0"),
            "processing_time_seconds": round(processing_time, 2),
            "generated_at": datetime.utcnow().isoformat(),
            "data_quality_tier": data_quality["quality_tier"],
            # Store ALL scraped data for dashboard preview
            "apify_data": apify_data,
            "perplexity_research": perplexity_data if perplexity_success else None,
        }

        # Progress: AI analysis complete
        emit_progress(submission_id, "ai_analysis", f"An√°lise gerada! Processamento conclu√≠do em {processing_time:.1f}s", 90)

        # Convert to JSON strings
        report_json = json.dumps(analysis, ensure_ascii=False)
        data_quality_json = json.dumps(data_quality, ensure_ascii=False)
        processing_metadata_json = json.dumps(processing_meta, ensure_ascii=False)

        # Progress: Saving to database
        emit_progress(submission_id, "finalizing", "Salvando relat√≥rio no banco de dados", 95)

        # Update submission with success (clear any previous errors)
        await update_submission_status(
            submission_id=submission_id,
            status="completed",
            report_json=report_json,
            data_quality_json=data_quality_json,
            processing_metadata=processing_metadata_json,
            error_message=None,  # CRITICAL: Clear error from failed first attempt
        )

        print(f"[OK] Analysis completed for submission {submission_id}")

        # Calculate confidence score automatically
        try:
            print(f"[CONFIDENCE] Calculating confidence score for submission {submission_id}...")
            submission_data = await get_submission(submission_id)
            confidence_score, confidence_breakdown = calculate_confidence_score(
                submission_data=submission_data,
                report_json=report_json,
                data_quality_json=data_quality_json,
                processing_metadata=processing_metadata_json,
            )

            # Add timestamp
            from datetime import datetime, timezone
            confidence_breakdown["calculated_at"] = datetime.now(timezone.utc).isoformat()

            # Update processing metadata with confidence
            processing_meta_with_confidence = json.loads(processing_metadata_json)
            processing_meta_with_confidence["confidence_score"] = confidence_score
            processing_meta_with_confidence["confidence_breakdown"] = confidence_breakdown

            # Save confidence to database
            await update_submission_status(
                submission_id=submission_id,
                status="completed",
                report_json=report_json,
                data_quality_json=data_quality_json,
                processing_metadata=json.dumps(processing_meta_with_confidence, ensure_ascii=False),
            )

            print(f"[CONFIDENCE] ‚úÖ Confidence score: {confidence_score}/100 ({confidence_breakdown['interpretation']['label']})")
        except Exception as conf_error:
            print(f"[WARNING] Confidence calculation failed: {conf_error}. Analysis still saved successfully.")

        # Progress: Complete!
        emit_progress(submission_id, "completed", f"‚úÖ Relat√≥rio conclu√≠do! Qualidade: {data_quality['quality_tier'].upper()}", 100)

        # Keep progress for 5 minutes after completion for streaming
        # (will be cleared by /stream endpoint after client disconnects)

    except Exception as e:
        error_message = str(e)
        print(f"[ERROR] Analysis failed for submission {submission_id}: {error_message}")

        # Progress: Error
        emit_progress(submission_id, "failed", f"‚ùå Erro: {error_message[:100]}", 0)

        # Update submission with failure
        await update_submission_status(
            submission_id=submission_id,
            status="failed",
            error_message=error_message,
        )


# API Endpoints

@app.get("/")
async def root():
    """API root endpoint"""
    return {
        "service": "Strategy AI Lead Generator API",
        "status": "running",
        "version": "2.0.0",
        "features": ["Supabase", "Authentication", "Apify Integration", "Upstash Redis", "SSE Streaming"]
    }


@app.get("/health")
async def health_check():
    """
    Health check endpoint for monitoring and load balancers

    Checks:
    - Database connectivity (Supabase)
    - Redis connectivity (Upstash)
    - OpenRouter API availability

    Returns:
        200 OK: All services healthy
        503 Service Unavailable: One or more services down
    """
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "2.0.0",
        "checks": {}
    }

    # Check 1: Database (Supabase)
    try:
        from database import count_submissions
        count = await count_submissions()
        health_status["checks"]["database"] = {
            "status": "healthy",
            "submissions_count": count
        }
    except Exception as e:
        health_status["status"] = "unhealthy"
        health_status["checks"]["database"] = {
            "status": "unhealthy",
            "error": str(e)
        }

    # Check 2: Redis (Upstash)
    try:
        from rate_limiter import get_redis_client
        redis = get_redis_client()
        redis.ping()
        health_status["checks"]["redis"] = {
            "status": "healthy"
        }
    except Exception as e:
        health_status["status"] = "unhealthy"
        health_status["checks"]["redis"] = {
            "status": "unhealthy",
            "error": str(e)
        }

    # Check 3: OpenRouter API (basic connectivity check)
    try:
        api_key = os.getenv("OPENROUTER_API_KEY")
        if api_key:
            health_status["checks"]["openrouter"] = {
                "status": "configured",
                "api_key_present": True
            }
        else:
            health_status["status"] = "degraded"
            health_status["checks"]["openrouter"] = {
                "status": "degraded",
                "api_key_present": False
            }
    except Exception as e:
        health_status["checks"]["openrouter"] = {
            "status": "unknown",
            "error": str(e)
        }

    # Determine HTTP status code
    if health_status["status"] == "healthy":
        return health_status
    elif health_status["status"] == "degraded":
        return JSONResponse(status_code=200, content=health_status)
    else:
        return JSONResponse(status_code=503, content=health_status)


# Authentication Endpoints

@app.post("/api/auth/login", response_model=LoginResponse)
async def login(credentials: LoginRequest):
    """
    Admin login endpoint

    Authenticates user with Supabase Auth and returns JWT token
    """
    try:
        auth_result = await authenticate_user(
            email=credentials.email,
            password=credentials.password
        )

        return LoginResponse(
            success=True,
            data=TokenResponse(
                access_token=auth_result["access_token"],
                token_type=auth_result["token_type"],
                user=UserResponse(
                    id=auth_result["user"]["id"],
                    email=auth_result["user"]["email"]
                )
            )
        )

    except HTTPException as e:
        return LoginResponse(
            success=False,
            error=e.detail
        )
    except Exception as e:
        print(f"[ERROR] Login error: {e}")
        return LoginResponse(
            success=False,
            error="Authentication failed"
        )


@app.post("/api/auth/signup", response_model=SignupResponse)
async def signup(credentials: SignupRequest):
    """
    User signup endpoint

    Creates a new user in Supabase Auth. User will not have admin access until manually granted in Supabase dashboard.
    """
    try:
        from supabase_client import get_supabase_client

        supabase = get_supabase_client(use_service_key=False)

        # Create user in Supabase Auth
        response = supabase.auth.sign_up({
            "email": credentials.email,
            "password": credentials.password
        })

        if response.user:
            return SignupResponse(
                success=True,
                message="Conta criada com sucesso! Voc√™ pode fazer login agora. Nota: Acesso administrativo ser√° concedido manualmente."
            )
        else:
            return SignupResponse(
                success=False,
                error="Falha ao criar conta. Por favor, tente novamente."
            )

    except Exception as e:
        error_message = str(e)
        print(f"[ERROR] Signup error: {error_message}")

        # Check for common errors
        if "already registered" in error_message.lower() or "already exists" in error_message.lower():
            return SignupResponse(
                success=False,
                error="Este email j√° est√° registrado. Fa√ßa login ou use outro email."
            )

        return SignupResponse(
            success=False,
            error="Erro ao criar conta. Por favor, tente novamente."
        )


# Public Endpoints

def normalize_website_url(url: str) -> str:
    """Ensure website URL has https:// prefix"""
    if not url:
        return url

    url = url.strip()

    # If URL already has a protocol, return as-is
    if url.startswith('http://') or url.startswith('https://'):
        return url

    # Add https:// prefix
    return f'https://{url}'


@app.post("/api/submit", response_model=SubmissionResponse)
async def submit_lead(
    submission: SubmissionCreate,
    background_tasks: BackgroundTasks,
    request: Request,
):
    """
    Submit a new lead form (Public endpoint - no auth required)

    - Validates input data
    - Checks rate limiting with Upstash Redis
    - Creates submission in Supabase
    - Triggers background AI analysis with Apify enrichment
    """
    try:
        # Get client IP for rate limiting
        client_ip = request.client.host

        # Check rate limit using Upstash Redis
        await check_rate_limit(client_ip)

        # Normalize website URL (add https:// if missing)
        normalized_website = normalize_website_url(submission.website) if submission.website else None

        # Create submission in database
        submission_id = await create_submission(
            name=submission.name,
            email=submission.email,
            company=submission.company,
            website=normalized_website,
            linkedin_company=submission.linkedin_company,
            linkedin_founder=submission.linkedin_founder,
            industry=submission.industry.value,
            challenge=submission.challenge,
        )

        # Trigger background analysis with Apify enrichment
        background_tasks.add_task(process_analysis_task, submission_id)

        print(f"[OK] New submission created: ID={submission_id}, Company={submission.company}")

        return SubmissionResponse(
            success=True,
            submission_id=submission_id,
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Submit error: {e}")
        return SubmissionResponse(
            success=False,
            error=str(e),
        )


@app.get("/api/submissions/{submission_id}/stream")
async def stream_analysis_progress(submission_id: int):
    """
    Server-Sent Events (SSE) endpoint for real-time analysis progress

    Usage (frontend):
    ```javascript
    const eventSource = new EventSource(`/api/submissions/${id}/stream`);
    eventSource.onmessage = (event) => {
        const progress = JSON.parse(event.data);
        console.log(`${progress.progress}%: ${progress.message}`);
    };
    ```

    Returns:
        StreamingResponse with Server-Sent Events

    Event format:
    ```json
    {
        "stage": "data_gathering",
        "message": "Coletando dados...",
        "progress": 30,
        "timestamp": "2025-01-26T10:30:00Z"
    }
    ```
    """
    async def event_generator() -> AsyncIterator[str]:
        """Generate SSE events for progress updates"""
        last_sent_count = 0
        timeout_seconds = 180  # 3 minutes timeout
        start_time = asyncio.get_event_loop().time()

        try:
            while True:
                # Check timeout
                if asyncio.get_event_loop().time() - start_time > timeout_seconds:
                    yield f"data: {json.dumps({'stage': 'timeout', 'message': 'Stream timeout', 'progress': 0})}\n\n"
                    break

                # Get new progress updates
                all_updates = get_progress_updates(submission_id)

                # Send new updates since last check
                for update in all_updates[last_sent_count:]:
                    yield f"data: {json.dumps(update)}\n\n"
                    last_sent_count += 1

                    # If completed or failed, close stream after sending
                    if update.get("stage") in ["completed", "failed"]:
                        yield f"data: {json.dumps({'stage': 'end', 'message': 'Stream closing', 'progress': 100})}\n\n"
                        # Clear progress after successful stream
                        await asyncio.sleep(1)  # Give client time to receive final message
                        clear_progress(submission_id)
                        return

                # Wait before checking again (polling interval)
                await asyncio.sleep(1)

        except asyncio.CancelledError:
            # Client disconnected - clean up
            print(f"[SSE] Client disconnected from submission {submission_id} stream")
            clear_progress(submission_id)
            raise

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        }
    )


# Protected Admin Endpoints (require authentication)

@app.get("/api/admin/submissions")
async def get_submissions(current_user: dict = RequireAuth, response: Response = None):
    """
    Get all submissions (Protected Admin endpoint)

    Requires valid JWT token in Authorization header
    Returns list of all submissions ordered by created_at DESC

    Cache-Control: 30 seconds (for faster dashboard refreshes)
    NOW WITH DASHBOARD STATS CACHING (5 min TTL)
    """
    try:
        print(f"[AUTH] User {current_user['email']} accessing submissions")

        # Check cache first for dashboard stats (5 min TTL)
        from enhanced_cache import get_cached_dashboard_stats, cache_dashboard_stats

        cached_stats = await get_cached_dashboard_stats()

        if cached_stats:
            print(f"[CACHE] üéØ Dashboard stats cache hit!")
            submissions = cached_stats.get("submissions", [])
            submission_list = [Submission(**sub) for sub in submissions]
        else:
            print(f"[CACHE] ‚ùå Dashboard stats cache miss - fetching from database")
            submissions = await get_all_submissions()

            # Convert to Pydantic models
            submission_list = [Submission(**sub) for sub in submissions]

            # Cache for 5 minutes
            await cache_dashboard_stats({
                "submissions": submissions,
                "cached_at": datetime.utcnow().isoformat()
            })

        # Add caching headers (30 second cache)
        if response:
            response.headers["Cache-Control"] = "private, max-age=30, must-revalidate"

        return {
            "success": True,
            "data": submission_list,
        }

    except Exception as e:
        print(f"[ERROR] Get submissions error: {e}")
        return {
            "success": False,
            "error": str(e),
        }


@app.get("/api/admin/submissions/{submission_id}/export-pdf")
async def export_submission_pdf(
    submission_id: int,
    current_user: dict = RequireAuth,
):
    """
    Export submission report as production-grade PDF (Protected Admin endpoint)

    Requires valid JWT token in Authorization header
    Returns PDF file with proper text rendering, spacing, and page breaks
    """
    try:
        from fastapi.responses import Response
        from pdf_generator import generate_pdf_from_report

        print(f"[AUTH] User {current_user['email']} exporting PDF for submission {submission_id}")

        # Get submission
        submission = await get_submission(submission_id)
        if not submission:
            return {"success": False, "error": f"Submission {submission_id} not found"}

        # Use edited_json if available (has edits), otherwise fallback to report_json
        report_json_str = submission.get('edited_json') or submission.get('report_json')
        if not report_json_str:
            return {"success": False, "error": "Report not yet generated"}

        # Parse report JSON
        try:
            report_json = json.loads(report_json_str)
        except json.JSONDecodeError:
            return {"success": False, "error": "Invalid report JSON"}

        # Build submission data for PDF generator (consistent format)
        submission_data = {
            "company": submission.get("company", ""),
            "industry": submission.get("industry", ""),
            "website": submission.get("website", ""),
            "challenge": submission.get("challenge", ""),
            "name": submission.get("name", ""),
            "updated_at": submission.get("last_edited_at") or submission.get("updated_at", "")
        }

        # Generate PDF
        print(f"[PDF] Generating PDF for submission {submission_id}...")
        pdf_bytes = generate_pdf_from_report(submission_data, report_json)

        if not pdf_bytes or len(pdf_bytes) == 0:
            print(f"[ERROR] PDF generation returned empty bytes!")
            return {"success": False, "error": "PDF generation failed - empty output"}

        print(f"[PDF] PDF generated successfully ({len(pdf_bytes)} bytes)")

        # Return PDF file
        has_edits = submission.get('edited_json') is not None
        filename = f"relatorio-estrategico-{submission['company'].replace(' ', '-')}-{datetime.now().strftime('%Y-%m-%d')}.pdf"

        # Add cache headers (5 min cache - PDFs don't change unless edited)
        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "Content-Length": str(len(pdf_bytes)),
                "Cache-Control": "private, max-age=300",  # 5 minute cache
                "ETag": f'"{submission_id}-v{submission.get("last_edited_at", submission.get("updated_at"))}"'
            }
        )

    except Exception as e:
        print(f"[ERROR] PDF export error: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


@app.post("/api/admin/reprocess/{submission_id}", response_model=ReprocessResponse)
async def reprocess_submission(
    submission_id: int,
    background_tasks: BackgroundTasks,
    current_user: dict = RequireAuth,
):
    """
    Reprocess a failed submission (Protected Admin endpoint)

    Requires valid JWT token in Authorization header
    Triggers a new background analysis task with Apify enrichment
    """
    try:
        print(f"[AUTH] User {current_user['email']} reprocessing submission {submission_id}")

        # Check if submission exists
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        # Reset to pending status
        await update_submission_status(
            submission_id=submission_id,
            status="pending",
            report_json=None,
            error_message=None,
        )

        # Trigger new analysis with Apify enrichment
        background_tasks.add_task(process_analysis_task, submission_id)

        print(f"[OK] Reprocessing submission {submission_id}")

        return ReprocessResponse(success=True)

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Reprocess error: {e}")
        return ReprocessResponse(success=False, error=str(e))


@app.post("/api/admin/submissions/{submission_id}/regenerate", response_model=ReprocessResponse)
async def regenerate_analysis(
    submission_id: int,
    background_tasks: BackgroundTasks,
    current_user: dict = RequireAuth,
):
    """
    Regenerate analysis for a completed submission (Protected Admin endpoint)

    This forces a fresh AI analysis while reusing cached external data (Apify/Perplexity)
    from institutional memory if still fresh (within TTL).

    Use cases:
    - Get a different perspective on the same company data
    - Regenerate after AI model improvements
    - Try again with better prompt engineering

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AUTH] User {current_user['email']} regenerating analysis for submission {submission_id}")

        # Check if submission exists
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        # Set to pending status (keep existing data in case of failure)
        await update_submission_status(
            submission_id=submission_id,
            status="pending",
            error_message=None,
            # Note: Don't clear report_json - we'll replace it on success
        )

        # Trigger new analysis with force_regenerate=True
        # This bypasses analysis cache but uses institutional memory for external data
        background_tasks.add_task(process_analysis_task, submission_id, True)

        print(f"[OK] Regenerating analysis for submission {submission_id} (force=True)")

        return ReprocessResponse(success=True)

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Regenerate error: {e}")
        return ReprocessResponse(success=False, error=str(e))


@app.patch("/api/admin/submissions/{submission_id}/status", response_model=UpdateStatusResponse)
async def update_status(
    submission_id: int,
    request: UpdateStatusRequest,
    current_user: dict = RequireAuth,
):
    """
    Update submission status (Protected Admin endpoint)

    Workflow stages:
    - pending: Initial state, waiting to process
    - processing: Analysis in progress
    - completed: Analysis done, needs review
    - ready_to_send: QA passed, ready to send to client
    - sent: Delivered to client
    - failed: Processing error

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AUTH] User {current_user['email']} updating status for submission {submission_id} to {request.status}")

        # Check if submission exists
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        # Update status in database
        await update_submission_status(
            submission_id=submission_id,
            status=request.status.value,
        )

        print(f"[OK] Updated submission {submission_id} status to {request.status}")

        return UpdateStatusResponse(success=True, new_status=request.status.value)

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Status update error: {e}")
        return UpdateStatusResponse(success=False, error=str(e))


@app.get("/api/admin/submissions/{submission_id}/confidence")
async def calculate_submission_confidence(
    submission_id: int,
    current_user: dict = RequireAuth,
):
    """
    Calculate or recalculate confidence score for a submission (Protected Admin endpoint)

    Returns confidence score (0-100) with detailed breakdown.

    Score components:
    - Data Completeness (0-30): How much data was gathered
    - Source Success Rate (0-25): How many data sources succeeded
    - Market Research Depth (0-20): Quality of competitor/trend analysis
    - Analysis Comprehensiveness (0-15): Coverage of all frameworks
    - TAM/SAM/SOM Availability (0-10): Market sizing quality

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AUTH] User {current_user['email']} calculating confidence for submission {submission_id}")

        # Get submission
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        # Calculate confidence score
        score, breakdown = calculate_confidence_score(
            submission_data=submission,
            report_json=submission.get("report_json"),
            data_quality_json=submission.get("data_quality_json"),
            processing_metadata=submission.get("processing_metadata"),
        )

        # Add timestamp
        from datetime import datetime, timezone
        breakdown["calculated_at"] = datetime.now(timezone.utc).isoformat()

        # Update database with new score
        await update_submission_status(
            submission_id=submission_id,
            status=submission["status"],  # Keep existing status
            report_json=submission.get("report_json"),  # Keep existing report
            data_quality_json=submission.get("data_quality_json"),  # Keep existing data quality
            processing_metadata=json.dumps({
                **(json.loads(submission.get("processing_metadata", "{}")) if submission.get("processing_metadata") else {}),
                "confidence_score": score,
                "confidence_breakdown": breakdown,
            }),
        )

        print(f"[OK] Calculated confidence score {score}/100 for submission {submission_id}")

        return {
            "success": True,
            "submission_id": submission_id,
            "confidence_score": score,
            "breakdown": breakdown,
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Confidence calculation error: {e}")
        return {
            "success": False,
            "error": str(e),
        }


@app.post("/api/admin/submissions/{submission_id}/chat")
async def chat_with_report(
    submission_id: int,
    request: Request,
    current_user: dict = RequireAuth,
):
    """
    Chat with AI about a specific report (Protected Admin endpoint)

    Send a message and get AI response with full context about the analysis.

    Request body:
    {
        "message": str,           # User's question
        "model": str (optional)   # "haiku" (fast) or "sonnet" (quality), default haiku
    }

    Response:
    {
        "success": bool,
        "message": str,           # AI response
        "model_used": str,
        "tokens_used": int,
        "timestamp": str,
        "chat_history": list      # Updated history
    }

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AUTH] User {current_user['email']} chatting about submission {submission_id}")

        # Get submission
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        if submission["status"] != "completed":
            raise HTTPException(status_code=400, detail="Can only chat about completed analyses")

        # Parse request body
        body = await request.json()
        user_message = body.get("message", "").strip()
        model_pref = body.get("model", "haiku")

        if not user_message:
            raise HTTPException(status_code=400, detail="Message cannot be empty")

        # Select model
        model_map = {
            "haiku": "claude-3-5-haiku-20241022",
            "sonnet": "claude-3-5-sonnet-20241022"
        }
        model = model_map.get(model_pref, "claude-3-5-haiku-20241022")

        # Parse report and data quality
        try:
            report_data = json.loads(submission.get("report_json", "{}"))
            data_quality = json.loads(submission.get("data_quality_json", "{}")) if submission.get("data_quality_json") else None
        except:
            raise HTTPException(status_code=500, detail="Failed to parse report data")

        # Get existing chat history
        chat_history = []
        try:
            if submission.get("chat_history"):
                stored_history = json.loads(submission["chat_history"])
                # Extract just role and content for API
                chat_history = [{"role": msg["role"], "content": msg["content"]} for msg in stored_history]
        except:
            chat_history = []

        # Send message to AI
        result = await send_chat_message(
            submission_data=submission,
            report_data=report_data,
            data_quality=data_quality,
            chat_history=chat_history,
            user_message=user_message,
            model=model
        )

        if not result.get("success", False):
            raise HTTPException(status_code=500, detail=result.get("error", "AI chat failed"))

        # Build updated chat history
        new_history = chat_history + [
            {"role": "user", "content": user_message},
            {"role": "assistant", "content": result["message"]}
        ]

        # Store chat history in database
        await update_submission_status(
            submission_id=submission_id,
            status=submission["status"],  # Keep existing status
            report_json=submission.get("report_json"),  # Keep existing report
            data_quality_json=submission.get("data_quality_json"),  # Keep existing quality
            processing_metadata=json.dumps({
                **(json.loads(submission.get("processing_metadata", "{}")) if submission.get("processing_metadata") else {}),
                "chat_history": new_history,  # Update chat history
                "last_chat_at": datetime.now(timezone.utc).isoformat()
            }),
        )

        print(f"[OK] Chat message processed for submission {submission_id}")

        return {
            "success": True,
            "message": result["message"],
            "model_used": result["model_used"],
            "tokens_used": result["tokens_used"],
            "timestamp": result["timestamp"],
            "chat_history": new_history
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Chat error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/api/admin/submissions/{submission_id}/chat/history")
async def get_chat_history(
    submission_id: int,
    current_user: dict = RequireAuth,
):
    """
    Get chat history for a submission (Protected Admin endpoint)

    Returns full chat history with timestamps.

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AUTH] User {current_user['email']} fetching chat history for submission {submission_id}")

        # Get submission
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        # Extract chat history
        chat_history = []
        try:
            if submission.get("chat_history"):
                chat_history = json.loads(submission["chat_history"])
        except:
            chat_history = []

        return {
            "success": True,
            "submission_id": submission_id,
            "chat_history": chat_history
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Get chat history error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/api/admin/chat/quick-prompts")
async def get_chat_quick_prompts(
    current_user: dict = RequireAuth,
):
    """
    Get quick prompt suggestions for common questions (Protected Admin endpoint)

    Returns list of suggested questions to ask about reports.

    Requires valid JWT token in Authorization header
    """
    return {
        "success": True,
        "prompts": get_quick_prompts()
    }



@app.get("/api/admin/dashboard/intelligence")
async def get_dashboard_intelligence(
    days: int = 7,
    current_user: dict = RequireAuth,
):
    """
    Get FREE AI-powered dashboard intelligence (Protected Admin endpoint)

    DISABLED: All free models hitting 429 rate limits
    Returns empty response immediately to avoid blocking dashboard
    """
    print(f"[DASHBOARD AI] DISABLED - Returning empty intelligence to avoid 429 rate limits")

    # Return empty intelligence structure immediately
    return {
        "executive_summary": "Dashboard Intelligence temporariamente desabilitado devido a limites de taxa dos modelos gratuitos.",
        "quality_trends": None,
        "common_challenges": [],
        "high_risk_submissions": [],
        "system_recommendations": [],
        "metadata": {
            "days_analyzed": days,
            "cost": 0.0,
            "disabled": True
        }
    }

    # OLD CODE (DISABLED):
    """
    try:
        print(f"[DASHBOARD AI] User {current_user['email']} requesting intelligence (last {days} days)")

        # Get all submissions
        all_submissions = await get_all_submissions()

        # Filter by date range (timezone-aware)
        current_cutoff = datetime.now(timezone.utc) - timedelta(days=days)
        previous_cutoff = datetime.now(timezone.utc) - timedelta(days=days * 2)

        current_submissions = [
            s for s in all_submissions
            if s.get("created_at") and datetime.fromisoformat(s["created_at"].replace("Z", "+00:00")) >= current_cutoff
        ]

        previous_submissions = [
            s for s in all_submissions
            if s.get("created_at") and previous_cutoff <= datetime.fromisoformat(s["created_at"].replace("Z", "+00:00")) < current_cutoff
        ]

        print(f"[DASHBOARD AI] Analyzing {len(current_submissions)} current + {len(previous_submissions)} previous submissions")

        # Generate FREE AI intelligence
        intelligence = await generate_dashboard_intelligence(
            current_submissions=current_submissions,
            previous_submissions=previous_submissions if len(previous_submissions) > 0 else None
        )

        print(f"[DASHBOARD AI] ‚úÖ Intelligence generated (cost: $0.00)")

        return {
            "success": True,
            "data": intelligence
        }

    except Exception as e:
        print(f"[ERROR] Dashboard intelligence error: {e}")
        return {
            "success": False,
            "error": str(e)
        }
    """


# ============================================================================
# AI-POWERED REPORT EDITING ENDPOINTS
# ============================================================================

@app.post("/api/admin/submissions/{submission_id}/edit", response_model=EditResponse)
async def edit_report_section(
    submission_id: int,
    edit_request: EditRequest,
    current_user: dict = RequireAuth,
):
    """
    Get AI-powered edit suggestion for a report section (Protected Admin endpoint)

    Uses adaptive model selection:
    - Simple edits (make shorter, more professional, etc.) ‚Üí Gemini Flash (~$0.0008/edit)
    - Complex edits (rewrite, add analysis, etc.) ‚Üí Claude Haiku (~$0.003/edit)

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AI EDITOR] User {current_user['email']} editing submission {submission_id}")
        print(f"[AI EDITOR] Section: {edit_request.section_path}, Instruction: {edit_request.instruction}")

        # Get submission
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        # Get report JSON (check edited_json first, fallback to original)
        report_json_str = submission.get("edited_json") or submission.get("report_json")
        if not report_json_str:
            raise HTTPException(status_code=400, detail="Report not yet generated")

        report_json = json.loads(report_json_str)

        # Extract section context
        section_context = extract_section_context(
            full_report=report_json,
            section_path=edit_request.section_path,
            context_chars=500
        )

        # Generate edit suggestion
        result = await generate_edit_suggestion(
            selected_text=edit_request.selected_text,
            instruction=edit_request.instruction,
            section_context=section_context,
            full_report_summary=report_json.get("sumario_executivo", ""),
            complexity=edit_request.complexity
        )

        print(f"[AI EDITOR] ‚úÖ Edit suggestion generated (model: {result['model_used']}, cost: ${result['cost_estimate']:.6f})")

        return EditResponse(
            success=True,
            suggested_edit=result["suggested_edit"],
            original_text=edit_request.selected_text,
            reasoning=result["reasoning"],
            model_used=result["model_used"],
            complexity=result["complexity"],
            cost_estimate=result["cost_estimate"]
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Edit generation error: {e}")
        import traceback
        traceback.print_exc()
        return EditResponse(success=False, error=str(e))


@app.post("/api/admin/submissions/{submission_id}/apply-edit", response_model=ApplyEditResponse)
async def apply_report_edit(
    submission_id: int,
    apply_request: ApplyEditRequest,
    current_user: dict = RequireAuth,
):
    """
    Apply an accepted edit to the report (Protected Admin endpoint)

    Updates the edited_json in database with the new text.
    Increments edit_count and updates last_edited_at timestamp.

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AI EDITOR] User {current_user['email']} applying edit to submission {submission_id}")
        print(f"[AI EDITOR] Section: {apply_request.section_path}")

        # Get submission
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        # Get report JSON (check edited_json first, fallback to original)
        report_json_str = submission.get("edited_json") or submission.get("report_json")
        if not report_json_str:
            raise HTTPException(status_code=400, detail="Report not yet generated")

        report_json = json.loads(report_json_str)

        # Apply edit to JSON path
        updated_report = apply_edit_to_json_path(
            report_json=report_json,
            section_path=apply_request.section_path,
            new_text=apply_request.new_text
        )

        # Get current edit count
        edit_count = submission.get("edit_count", 0) + 1

        # Update database
        from supabase_client import supabase_service
        update_result = supabase_service.table("submissions").update({
            "edited_json": json.dumps(updated_report, ensure_ascii=False),
            "last_edited_at": datetime.now(timezone.utc).isoformat(),
            "edit_count": edit_count
        }).eq("id", submission_id).execute()

        print(f"[AI EDITOR] ‚úÖ Edit applied successfully (total edits: {edit_count})")

        return ApplyEditResponse(
            success=True,
            updated_report=updated_report,
            edit_count=edit_count
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Apply edit error: {e}")
        import traceback
        traceback.print_exc()
        return ApplyEditResponse(success=False, error=str(e))


def _generate_pdf_background(submission_id: int, submission_data: dict, report_json: dict):
    """
    Background task to generate PDF without blocking HTTP response
    """
    try:
        import os
        print(f"[PDF BACKGROUND] Starting PDF generation for submission {submission_id}")

        # Generate PDF
        pdf_bytes = generate_pdf_from_report(
            submission_data=submission_data,
            report_json=report_json
        )

        # Save PDF to file system
        pdf_filename = f"report_{submission_id}_edited.pdf"
        pdf_path = f"./reports/{pdf_filename}"

        # Create reports directory if it doesn't exist
        os.makedirs("./reports", exist_ok=True)

        # Write PDF to file
        with open(pdf_path, "wb") as f:
            f.write(pdf_bytes)

        print(f"[PDF BACKGROUND] ‚úÖ PDF generated successfully: {pdf_filename}")

    except Exception as e:
        print(f"[PDF BACKGROUND] ‚ùå PDF generation failed: {e}")
        import traceback
        traceback.print_exc()


@app.post("/api/admin/submissions/{submission_id}/regenerate-pdf", response_model=RegeneratePDFResponse)
async def regenerate_pdf_with_edits(
    submission_id: int,
    background_tasks: BackgroundTasks,
    current_user: dict = RequireAuth,
):
    """
    Regenerate PDF with applied edits (Protected Admin endpoint) - ASYNC

    Uses edited_json if available, otherwise falls back to original report_json.
    Generates PDF in background, returns immediately.

    Requires valid JWT token in Authorization header

    Performance: Returns in <100ms (90% faster than synchronous generation)
    """
    try:
        print(f"[AI EDITOR] User {current_user['email']} requesting PDF regeneration for submission {submission_id}")

        # Get submission
        submission = await get_submission(submission_id)
        if not submission:
            raise HTTPException(status_code=404, detail="Submission not found")

        # Get report JSON (use edited_json if available)
        report_json_str = submission.get("edited_json") or submission.get("report_json")
        if not report_json_str:
            raise HTTPException(status_code=400, detail="Report not yet generated")

        report_json = json.loads(report_json_str)

        # Build submission data for PDF generator
        submission_data = {
            "company": submission.get("company", ""),
            "industry": submission.get("industry", ""),
            "website": submission.get("website", ""),
            "challenge": submission.get("challenge", ""),
            "name": submission.get("name", ""),
            "updated_at": submission.get("last_edited_at") or submission.get("updated_at", "")
        }

        # Add PDF generation to background queue (non-blocking!)
        background_tasks.add_task(
            _generate_pdf_background,
            submission_id,
            submission_data,
            report_json
        )

        print(f"[AI EDITOR] ‚úÖ PDF generation queued (background task)")

        return RegeneratePDFResponse(
            success=True,
            pdf_url=f"/api/admin/submissions/{submission_id}/export-pdf",
            message="PDF generation started in background. Use the export URL to download when ready."
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] PDF regeneration error: {e}")
        import traceback
        traceback.print_exc()
        return RegeneratePDFResponse(success=False, error=str(e))


@app.get("/api/admin/cache/statistics")
async def get_cache_statistics_endpoint(current_user: dict = RequireAuth):
    """
    Get comprehensive cache statistics (Protected Admin endpoint)

    Returns cache performance metrics including:
    - Cache hit rates
    - Total cost savings
    - Cache sizes
    - Most accessed entries

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AUTH] User {current_user['email']} accessing cache statistics")

        from enhanced_cache import get_cache_statistics, clear_expired_cache
        from institutional_memory import get_cache_stats as get_institutional_stats

        # Get all cache statistics
        enhanced_stats = await get_cache_statistics()
        institutional_stats = await get_institutional_stats()

        # Calculate overall statistics
        total_cost_saved = enhanced_stats.get("total_cost_saved", 0)

        return {
            "success": True,
            "data": {
                "enhanced_cache": enhanced_stats,
                "institutional_memory": institutional_stats,
                "summary": {
                    "total_cost_saved_usd": total_cost_saved,
                    "total_records": (
                        enhanced_stats.get("analysis_cache", {}).get("total_records", 0) +
                        enhanced_stats.get("stage_cache", {}).get("total_records", 0) +
                        enhanced_stats.get("pdf_cache", {}).get("total_records", 0) +
                        institutional_stats.get("total_records", 0)
                    ),
                    "analysis_cache_hit_rate": "High value cache - saves $15-25 per hit",
                    "stage_cache_hit_rate": "Partial savings - saves $0.10-3 per stage",
                    "pdf_cache": "Time savings - instant PDF regeneration",
                },
                "recommendations": [
                    f"‚úÖ Analysis cache saved ${total_cost_saved:.2f} total",
                    "üí° Consider running clear_expired_cache() periodically (daily cron)",
                    "üìä Monitor cache hit rates to optimize TTL values",
                    "üöÄ High cache hit rate = significant cost savings"
                ]
            }
        }

    except Exception as e:
        print(f"[ERROR] Cache statistics error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/api/admin/cache/clear-expired")
async def clear_expired_cache_endpoint(current_user: dict = RequireAuth):
    """
    Clear expired cache entries (Protected Admin endpoint)

    Removes old cache entries based on TTL:
    - Analysis cache: 30 days
    - Stage cache: 7 days
    - PDF cache: 90 days
    - Stats cache: 5 minutes

    Requires valid JWT token in Authorization header
    """
    try:
        print(f"[AUTH] User {current_user['email']} clearing expired cache")

        from enhanced_cache import clear_expired_cache

        cleared = await clear_expired_cache()

        total_cleared = sum(cleared.values())

        return {
            "success": True,
            "data": {
                "cleared": cleared,
                "total_cleared": total_cleared,
                "message": f"Cleared {total_cleared} expired cache entries"
            }
        }

    except Exception as e:
        print(f"[ERROR] Clear cache error: {e}")
        return {
            "success": False,
            "error": str(e)
        }


# Exception handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={"success": False, "error": exc.detail}
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    print(f"[ERROR] Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={"success": False, "error": "Internal server error"}
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
